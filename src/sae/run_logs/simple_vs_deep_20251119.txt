Simple:

============================================================
SIMPLE SAE EXPERIMENT
============================================================

[1/7] Loading data...
Loaded 1000 texts

[2/7] Loading model...
Using device: cuda

[3/7] Extracting activations...
Extracted activations: torch.Size([109182, 512])
  - 109,182 token activations
  - 512 dimensions

[4/7] Creating SAE...
SAE architecture: 512 → 16384 → 512
Sparsity: TopKSparsity

[4.5/7] Creating optimizer...
SAE optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

[5/7] Creating trainer...

[6/7] Training SAE...

Training SAE on 109,182 activation vectors
Input dim: 512, Latent dim: 16384
Batch size: 32, Epochs: 20
============================================================
Epoch   1/20 | Loss: 0.0944 | Recon: 0.0944 | Active: 32/16384 (0.2%) | Time: 12.36s
Epoch   2/20 | Loss: 0.0565 | Recon: 0.0565 | Active: 32/16384 (0.2%) | Time: 12.50s
Epoch   3/20 | Loss: 0.0540 | Recon: 0.0540 | Active: 32/16384 (0.2%) | Time: 12.70s
Epoch   4/20 | Loss: 0.0528 | Recon: 0.0528 | Active: 32/16384 (0.2%) | Time: 12.79s
Epoch   5/20 | Loss: 0.0521 | Recon: 0.0521 | Active: 32/16384 (0.2%) | Time: 12.22s
Epoch   6/20 | Loss: 0.0517 | Recon: 0.0517 | Active: 32/16384 (0.2%) | Time: 14.08s
Epoch   7/20 | Loss: 0.0514 | Recon: 0.0514 | Active: 32/16384 (0.2%) | Time: 14.15s
Epoch   8/20 | Loss: 0.0509 | Recon: 0.0509 | Active: 32/16384 (0.2%) | Time: 14.34s
Epoch   9/20 | Loss: 0.0508 | Recon: 0.0508 | Active: 32/16384 (0.2%) | Time: 14.27s
Epoch  10/20 | Loss: 0.0507 | Recon: 0.0507 | Active: 32/16384 (0.2%) | Time: 14.04s
Epoch  11/20 | Loss: 0.0506 | Recon: 0.0506 | Active: 32/16384 (0.2%) | Time: 14.10s
Epoch  12/20 | Loss: 0.0504 | Recon: 0.0504 | Active: 32/16384 (0.2%) | Time: 12.49s
Epoch  13/20 | Loss: 0.0504 | Recon: 0.0504 | Active: 32/16384 (0.2%) | Time: 12.40s
Epoch  14/20 | Loss: 0.0503 | Recon: 0.0503 | Active: 32/16384 (0.2%) | Time: 12.51s
Epoch  15/20 | Loss: 0.0502 | Recon: 0.0502 | Active: 32/16384 (0.2%) | Time: 10.81s
Epoch  16/20 | Loss: 0.0501 | Recon: 0.0501 | Active: 32/16384 (0.2%) | Time: 10.60s
Epoch  17/20 | Loss: 0.0501 | Recon: 0.0501 | Active: 32/16384 (0.2%) | Time: 11.12s
Epoch  18/20 | Loss: 0.0500 | Recon: 0.0500 | Active: 32/16384 (0.2%) | Time: 10.46s
Epoch  19/20 | Loss: 0.0500 | Recon: 0.0500 | Active: 32/16384 (0.2%) | Time: 10.43s
Epoch  20/20 | Loss: 0.0498 | Recon: 0.0498 | Active: 32/16384 (0.2%) | Time: 10.90s
============================================================
Training complete!


[7/7] Evaluating SAE...

============================================================
FEATURE ANALYSIS
============================================================

Text: 'Dogs are man's best friend.'
  Active features: 111/16384 (0.7%)
  Top 10 features:
    Feature 3455: 15.846
    Feature 14717: 10.891
    Feature 16376: 10.303
    Feature 13337: 2.493
    Feature 15356: 1.566
    Feature 6192: 0.994
    Feature 7464: 0.948
    Feature 4033: 0.775
    Feature 6577: 0.730
    Feature 12727: 0.674

Text: 'Neural networks process information.'
  Active features: 77/16384 (0.5%)
  Top 10 features:
    Feature 3455: 28.447
    Feature 14717: 20.898
    Feature 16376: 15.793
    Feature 13337: 4.172
    Feature 631: 1.876
    Feature 6577: 1.519
    Feature 15356: 1.279
    Feature 12418: 0.887
    Feature 9384: 0.754
    Feature 13624: 0.725

Text: 'London is a major city in England.'
  Active features: 101/16384 (0.6%)
  Top 10 features:
    Feature 3455: 15.844
    Feature 14717: 10.885
    Feature 16376: 9.955
    Feature 13337: 2.674
    Feature 12418: 1.605
    Feature 15356: 1.457
    Feature 6192: 1.383
    Feature 7805: 1.043
    Feature 13709: 1.030
    Feature 1745: 0.882
============================================================

Saving checkpoint...
✓ Checkpoint saved to: C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\checkpoints\simple_sae_layer3
  - Weights: sae_weights.pt
  - Activation mean: activation_mean.pt
  - Config: config.json

============================================================
EXPERIMENT COMPLETE!
============================================================


============================================================
============================================================
============================================================
Deep
============================================================
============================================================
============================================================


============================================================
DEEP SAE EXPERIMENT
============================================================

[1/7] Loading data...
Loaded 1000 texts

[2/7] Loading model...
Using device: cuda

[3/7] Extracting activations...
Extracted activations: torch.Size([109182, 512])
  - 109,182 token activations
  - 512 dimensions

[4/7] Creating Deep SAE...
Deep SAE architecture:
  Encoder: 512 → 2048 → 3584
  Latent: 3584 dimensions (sparse)
  Decoder: 3584 → 2048 → 512
  Sparsity: TopKSparsity

[4.5/7] Creating optimizer...
SAE optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

[5/7] Creating trainer...

[6/7] Training Deep SAE...

Training SAE on 109,182 activation vectors
Input dim: 512, Latent dim: 3584
Batch size: 32, Epochs: 20
============================================================
Epoch   1/20 | Loss: 0.1081 | Recon: 0.1071 | Active: 32/3584 (0.9%) | Time: 16.98s
Epoch   2/20 | Loss: 0.0736 | Recon: 0.0727 | Active: 32/3584 (0.9%) | Time: 16.92s
Epoch   3/20 | Loss: 0.0659 | Recon: 0.0651 | Active: 32/3584 (0.9%) | Time: 16.67s
Epoch   4/20 | Loss: 0.0612 | Recon: 0.0604 | Active: 32/3584 (0.9%) | Time: 17.38s
Epoch   5/20 | Loss: 0.0590 | Recon: 0.0584 | Active: 32/3584 (0.9%) | Time: 17.83s
Epoch   6/20 | Loss: 0.0566 | Recon: 0.0561 | Active: 32/3584 (0.9%) | Time: 18.05s
Epoch   7/20 | Loss: 0.0555 | Recon: 0.0550 | Active: 32/3584 (0.9%) | Time: 17.38s
Epoch   8/20 | Loss: 0.0539 | Recon: 0.0534 | Active: 32/3584 (0.9%) | Time: 17.53s
Epoch   9/20 | Loss: 0.0529 | Recon: 0.0525 | Active: 32/3584 (0.9%) | Time: 17.40s
Epoch  10/20 | Loss: 0.0525 | Recon: 0.0522 | Active: 32/3584 (0.9%) | Time: 16.91s
Epoch  11/20 | Loss: 0.0515 | Recon: 0.0512 | Active: 32/3584 (0.9%) | Time: 18.73s
Epoch  12/20 | Loss: 0.0509 | Recon: 0.0506 | Active: 32/3584 (0.9%) | Time: 17.30s
Epoch  13/20 | Loss: 0.0504 | Recon: 0.0502 | Active: 32/3584 (0.9%) | Time: 17.42s
Epoch  14/20 | Loss: 0.0500 | Recon: 0.0498 | Active: 32/3584 (0.9%) | Time: 16.75s
Epoch  15/20 | Loss: 0.0495 | Recon: 0.0493 | Active: 32/3584 (0.9%) | Time: 16.95s
Epoch  16/20 | Loss: 0.0494 | Recon: 0.0492 | Active: 32/3584 (0.9%) | Time: 17.51s
Epoch  17/20 | Loss: 0.0488 | Recon: 0.0487 | Active: 32/3584 (0.9%) | Time: 17.83s
Epoch  18/20 | Loss: 0.0489 | Recon: 0.0487 | Active: 32/3584 (0.9%) | Time: 18.03s
Epoch  19/20 | Loss: 0.0484 | Recon: 0.0482 | Active: 32/3584 (0.9%) | Time: 18.29s
Epoch  20/20 | Loss: 0.0482 | Recon: 0.0480 | Active: 32/3584 (0.9%) | Time: 18.39s
============================================================
Training complete!


[7/7] Evaluating Deep SAE...

============================================================
FEATURE ANALYSIS
============================================================

Text: 'Dogs are man's best friend.'
  Active features: 83/3584 (2.3%)
  Top 10 features:
    Feature 1675: 806.195
    Feature 668: 568.297
    Feature 2013: 532.267
    Feature 388: 504.758
    Feature 1413: 475.123
    Feature 283: 474.635
    Feature 3490: 440.141
    Feature 1495: 438.489
    Feature 58: 427.354
    Feature 3306: 422.339

Text: 'Neural networks process information.'
  Active features: 68/3584 (1.9%)
  Top 10 features:
    Feature 1675: 1209.388
    Feature 668: 1068.644
    Feature 2013: 1039.110
    Feature 388: 960.096
    Feature 1413: 787.768
    Feature 3490: 769.030
    Feature 283: 757.467
    Feature 3306: 709.400
    Feature 1495: 666.065
    Feature 1432: 651.003

Text: 'London is a major city in England.'
  Active features: 89/3584 (2.5%)
  Top 10 features:
    Feature 1675: 781.502
    Feature 668: 607.904
    Feature 2013: 533.474
    Feature 388: 517.484
    Feature 1413: 466.544
    Feature 283: 462.775
    Feature 3490: 444.379
    Feature 1495: 424.833
    Feature 3306: 420.790
    Feature 58: 404.428
============================================================

Saving checkpoint...
✓ Checkpoint saved to: C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\checkpoints\deep_sae_layer3
  - Weights: sae_weights.pt
  - Activation mean: activation_mean.pt
  - Config: config.json

============================================================
EXPERIMENT COMPLETE!
============================================================

Deep SAE trained successfully!
Architecture: 2-layer encoder, 1-layer decoder
Latent dimension: 3,584
Final reconstruction loss: 0.0480
Final sparsity: 0.9% active features


============================================================
DEEP SAE EXPERIMENT
============================================================

[1/7] Loading data...
Loaded 1000 texts

[2/7] Loading model...
Using device: cuda

[3/7] Extracting activations...
Extracted activations: torch.Size([109182, 512])
  - 109,182 token activations
  - 512 dimensions

[4/7] Creating Deep SAE...
Deep SAE architecture:
  Encoder: 512 → 2048 → 16384
  Latent: 16384 dimensions (sparse)
  Decoder: 16384 → 2048 → 512
  Sparsity: TopKSparsity

[4.5/7] Creating optimizer...
SAE optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

[5/7] Creating trainer...

[6/7] Training Deep SAE...

Training SAE on 109,182 activation vectors
Input dim: 512, Latent dim: 16384
Batch size: 32, Epochs: 20
============================================================
Epoch   1/20 | Loss: 0.1145 | Recon: 0.1135 | Active: 32/16384 (0.2%) | Time: 49.99s
Epoch   2/20 | Loss: 0.0790 | Recon: 0.0780 | Active: 32/16384 (0.2%) | Time: 51.69s
Epoch   3/20 | Loss: 0.0686 | Recon: 0.0677 | Active: 32/16384 (0.2%) | Time: 51.94s
Epoch   4/20 | Loss: 0.0650 | Recon: 0.0643 | Active: 32/16384 (0.2%) | Time: 52.10s
Epoch   5/20 | Loss: 0.0603 | Recon: 0.0596 | Active: 32/16384 (0.2%) | Time: 51.91s
Epoch   6/20 | Loss: 0.0587 | Recon: 0.0581 | Active: 32/16384 (0.2%) | Time: 51.79s
Epoch   7/20 | Loss: 0.0575 | Recon: 0.0570 | Active: 32/16384 (0.2%) | Time: 51.88s
Epoch   8/20 | Loss: 0.0557 | Recon: 0.0552 | Active: 32/16384 (0.2%) | Time: 51.82s
Epoch   9/20 | Loss: 0.0547 | Recon: 0.0543 | Active: 32/16384 (0.2%) | Time: 52.09s
Epoch  10/20 | Loss: 0.0539 | Recon: 0.0536 | Active: 32/16384 (0.2%) | Time: 52.16s
Epoch  11/20 | Loss: 0.0531 | Recon: 0.0528 | Active: 32/16384 (0.2%) | Time: 51.98s
Epoch  12/20 | Loss: 0.0526 | Recon: 0.0523 | Active: 32/16384 (0.2%) | Time: 51.96s
Epoch  13/20 | Loss: 0.0521 | Recon: 0.0519 | Active: 32/16384 (0.2%) | Time: 52.07s
Epoch  14/20 | Loss: 0.0514 | Recon: 0.0512 | Active: 32/16384 (0.2%) | Time: 52.11s
Epoch  15/20 | Loss: 0.0509 | Recon: 0.0507 | Active: 32/16384 (0.2%) | Time: 50.55s
Epoch  16/20 | Loss: 0.0506 | Recon: 0.0504 | Active: 32/16384 (0.2%) | Time: 51.46s
Epoch  17/20 | Loss: 0.0503 | Recon: 0.0501 | Active: 32/16384 (0.2%) | Time: 53.20s
Epoch  18/20 | Loss: 0.0498 | Recon: 0.0496 | Active: 32/16384 (0.2%) | Time: 54.88s
Epoch  19/20 | Loss: 0.0496 | Recon: 0.0494 | Active: 32/16384 (0.2%) | Time: 54.73s
Epoch  20/20 | Loss: 0.0492 | Recon: 0.0491 | Active: 32/16384 (0.2%) | Time: 54.72s
============================================================
Training complete!


[7/7] Evaluating Deep SAE...

============================================================
FEATURE ANALYSIS
============================================================

Text: 'Dogs are man's best friend.'
  Active features: 81/16384 (0.5%)
  Top 10 features:
    Feature 9414: 639.286
    Feature 14243: 495.286
    Feature 12879: 462.350
    Feature 482: 459.311
    Feature 14692: 453.147
    Feature 75: 444.980
    Feature 11848: 416.311
    Feature 7014: 411.287
    Feature 5881: 391.653
    Feature 10411: 350.904

Text: 'Neural networks process information.'
  Active features: 81/16384 (0.5%)
  Top 10 features:
    Feature 9414: 1018.406
    Feature 14243: 893.383
    Feature 75: 846.081
    Feature 12879: 842.901
    Feature 14692: 758.734
    Feature 10411: 736.798
    Feature 11848: 715.869
    Feature 7014: 600.903
    Feature 5881: 596.179
    Feature 7815: 563.754

Text: 'London is a major city in England.'
  Active features: 89/16384 (0.5%)
  Top 10 features:
    Feature 9414: 909.863
    Feature 14243: 497.482
    Feature 12879: 448.177
    Feature 10411: 446.600
    Feature 14692: 437.730
    Feature 7014: 427.132
    Feature 11848: 423.946
    Feature 75: 408.586
    Feature 482: 397.668
    Feature 5881: 370.312
============================================================

Saving checkpoint...
✓ Checkpoint saved to: C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\checkpoints\deep_sae_layer3
  - Weights: sae_weights.pt
  - Activation mean: activation_mean.pt
  - Config: config.json

============================================================
EXPERIMENT COMPLETE!
============================================================

Deep SAE trained successfully!
Architecture: 2-layer encoder, 2-layer decoder
Latent dimension: 16,384
Final reconstruction loss: 0.0491
Final sparsity: 0.2% active features

C:\Users\paras\miniconda3\envs\SAE\python.exe C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\deep_sae_experiment.py
============================================================
DEEP SAE EXPERIMENT
============================================================

[1/7] Loading data...
Loaded 1000 texts

[2/7] Loading model...
Using device: cuda

[3/7] Extracting activations...
Extracted activations: torch.Size([109182, 512])
  - 109,182 token activations
  - 512 dimensions

[4/7] Creating Deep SAE...
Deep SAE architecture:
  Encoder: 512 → 16384
  Latent: 16384 dimensions (sparse)
  Decoder: 16384 →  → 512
  Sparsity: TopKSparsity

[4.5/7] Creating optimizer...
SAE optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

[5/7] Creating trainer...

[6/7] Training Deep SAE...

Training SAE on 109,182 activation vectors
Input dim: 512, Latent dim: 16384
Batch size: 32, Epochs: 20
============================================================
Epoch   1/20 | Loss: 0.0936 | Recon: 0.0922 | Active: 32/16384 (0.2%) | Time: 15.10s
Epoch   2/20 | Loss: 0.0564 | Recon: 0.0551 | Active: 32/16384 (0.2%) | Time: 15.00s
Epoch   3/20 | Loss: 0.0520 | Recon: 0.0510 | Active: 32/16384 (0.2%) | Time: 15.10s
Epoch   4/20 | Loss: 0.0483 | Recon: 0.0476 | Active: 32/16384 (0.2%) | Time: 14.71s
Epoch   5/20 | Loss: 0.0477 | Recon: 0.0470 | Active: 32/16384 (0.2%) | Time: 15.71s
Epoch   6/20 | Loss: 0.0456 | Recon: 0.0451 | Active: 32/16384 (0.2%) | Time: 17.60s
Epoch   7/20 | Loss: 0.0451 | Recon: 0.0446 | Active: 32/16384 (0.2%) | Time: 15.93s
Epoch   8/20 | Loss: 0.0446 | Recon: 0.0442 | Active: 32/16384 (0.2%) | Time: 15.82s
Epoch   9/20 | Loss: 0.0440 | Recon: 0.0437 | Active: 32/16384 (0.2%) | Time: 18.09s
Epoch  10/20 | Loss: 0.0435 | Recon: 0.0431 | Active: 32/16384 (0.2%) | Time: 15.76s
Epoch  11/20 | Loss: 0.0442 | Recon: 0.0439 | Active: 32/16384 (0.2%) | Time: 16.37s
Epoch  12/20 | Loss: 0.0428 | Recon: 0.0425 | Active: 32/16384 (0.2%) | Time: 16.66s
Epoch  13/20 | Loss: 0.0428 | Recon: 0.0425 | Active: 32/16384 (0.2%) | Time: 16.52s
Epoch  14/20 | Loss: 0.0422 | Recon: 0.0420 | Active: 32/16384 (0.2%) | Time: 15.28s
Epoch  15/20 | Loss: 0.0421 | Recon: 0.0419 | Active: 32/16384 (0.2%) | Time: 15.49s
Epoch  16/20 | Loss: 0.0418 | Recon: 0.0416 | Active: 32/16384 (0.2%) | Time: 15.75s
Epoch  17/20 | Loss: 0.0421 | Recon: 0.0419 | Active: 32/16384 (0.2%) | Time: 16.06s
Epoch  18/20 | Loss: 0.0416 | Recon: 0.0414 | Active: 32/16384 (0.2%) | Time: 15.89s
Epoch  19/20 | Loss: 0.0415 | Recon: 0.0414 | Active: 32/16384 (0.2%) | Time: 15.35s
Epoch  20/20 | Loss: 0.0414 | Recon: 0.0412 | Active: 32/16384 (0.2%) | Time: 15.45s
============================================================
Training complete!


[7/7] Evaluating Deep SAE...

============================================================
FEATURE ANALYSIS
============================================================

Text: 'Dogs are man's best friend.'
  Active features: 147/16384 (0.9%)
  Top 10 features:
    Feature 11414: 48.882
    Feature 3299: 43.120
    Feature 14736: 39.855
    Feature 2275: 38.212
    Feature 2387: 37.902
    Feature 13330: 37.766
    Feature 14381: 36.244
    Feature 14802: 34.759
    Feature 10401: 33.903
    Feature 12063: 32.840

Text: 'Neural networks process information.'
  Active features: 86/16384 (0.5%)
  Top 10 features:
    Feature 11414: 98.375
    Feature 3299: 85.261
    Feature 2387: 65.820
    Feature 13330: 64.887
    Feature 2275: 64.302
    Feature 14381: 63.315
    Feature 14736: 62.883
    Feature 14669: 62.470
    Feature 12063: 62.149
    Feature 429: 59.087

Text: 'London is a major city in England.'
  Active features: 136/16384 (0.8%)
  Top 10 features:
    Feature 11414: 50.197
    Feature 3299: 45.180
    Feature 14736: 41.433
    Feature 2387: 39.819
    Feature 2275: 38.975
    Feature 13330: 37.286
    Feature 14381: 36.714
    Feature 12063: 34.541
    Feature 10401: 33.884
    Feature 14802: 33.378
============================================================

Saving checkpoint...
✓ Checkpoint saved to: C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\checkpoints\deep_sae_layer3
  - Weights: sae_weights.pt
  - Activation mean: activation_mean.pt
  - Config: config.json

============================================================
EXPERIMENT COMPLETE!
============================================================

Deep SAE trained successfully!
Architecture: 1-layer encoder, 1-layer decoder
Latent dimension: 16,384
Final reconstruction loss: 0.0412
Final sparsity: 0.2% active features

Process finished with exit code 0


C:\Users\paras\miniconda3\envs\SAE\python.exe C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\deep_sae_experiment.py
============================================================
DEEP SAE EXPERIMENT
============================================================

[1/7] Loading data...
Loaded 1024 texts

[2/7] Loading model...
Using device: cuda

[3/7] Extracting activations...
Extracted activations: torch.Size([111670, 512])
  - 111,670 token activations
  - 512 dimensions

[4/7] Creating Deep SAE...
Deep SAE architecture:
  Encoder: 512 → 2048 → 16384
  Latent: 16384 dimensions (sparse)
  Decoder: 16384 → 2048 → 512
  Sparsity: TopKSparsity

[4.5/7] Creating optimizer...
SAE optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    decoupled_weight_decay: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)

[5/7] Creating trainer...

[6/7] Training Deep SAE...

Training SAE on 111,670 activation vectors
Input dim: 512, Latent dim: 16384
Batch size: 32, Epochs: 20
============================================================
Epoch   1/20 | Loss: 0.1234 | Recon: 0.1224 | Active: 64/16384 (0.4%) | Time: 50.13s
Epoch   2/20 | Loss: 0.0805 | Recon: 0.0795 | Active: 64/16384 (0.4%) | Time: 54.14s
Epoch   3/20 | Loss: 0.0592 | Recon: 0.0583 | Active: 64/16384 (0.4%) | Time: 56.28s
Epoch   4/20 | Loss: 0.0530 | Recon: 0.0522 | Active: 64/16384 (0.4%) | Time: 53.35s
Epoch   5/20 | Loss: 0.0484 | Recon: 0.0477 | Active: 64/16384 (0.4%) | Time: 53.56s
Epoch   6/20 | Loss: 0.0454 | Recon: 0.0449 | Active: 64/16384 (0.4%) | Time: 55.55s
Epoch   7/20 | Loss: 0.0438 | Recon: 0.0433 | Active: 64/16384 (0.4%) | Time: 55.35s
Epoch   8/20 | Loss: 0.0422 | Recon: 0.0418 | Active: 64/16384 (0.4%) | Time: 54.33s
Epoch   9/20 | Loss: 0.0410 | Recon: 0.0407 | Active: 64/16384 (0.4%) | Time: 56.08s
Epoch  10/20 | Loss: 0.0400 | Recon: 0.0396 | Active: 64/16384 (0.4%) | Time: 56.82s
Epoch  11/20 | Loss: 0.0392 | Recon: 0.0389 | Active: 64/16384 (0.4%) | Time: 54.83s
Epoch  12/20 | Loss: 0.0384 | Recon: 0.0382 | Active: 64/16384 (0.4%) | Time: 53.90s
Epoch  13/20 | Loss: 0.0378 | Recon: 0.0376 | Active: 64/16384 (0.4%) | Time: 54.86s
Epoch  14/20 | Loss: 0.0373 | Recon: 0.0371 | Active: 64/16384 (0.4%) | Time: 54.05s
Epoch  15/20 | Loss: 0.0368 | Recon: 0.0366 | Active: 64/16384 (0.4%) | Time: 55.27s
Epoch  16/20 | Loss: 0.0364 | Recon: 0.0363 | Active: 64/16384 (0.4%) | Time: 54.39s
Epoch  17/20 | Loss: 0.0361 | Recon: 0.0360 | Active: 64/16384 (0.4%) | Time: 53.48s
Epoch  18/20 | Loss: 0.0359 | Recon: 0.0357 | Active: 64/16384 (0.4%) | Time: 55.29s
Epoch  19/20 | Loss: 0.0357 | Recon: 0.0356 | Active: 64/16384 (0.4%) | Time: 56.86s
Epoch  20/20 | Loss: 0.0356 | Recon: 0.0355 | Active: 64/16384 (0.4%) | Time: 56.64s
============================================================
Training complete!


[7/7] Evaluating Deep SAE...

============================================================
FEATURE ANALYSIS
============================================================

Text: 'Dogs are man's best friend.'
  Active features: 149/16384 (0.9%)
  Top 10 features:
    Feature 13092: 272.617
    Feature 12441: 266.961
    Feature 12315: 254.879
    Feature 5603: 253.492
    Feature 9061: 224.479
    Feature 10253: 219.204
    Feature 7915: 203.425
    Feature 11872: 165.272
    Feature 11322: 154.500
    Feature 4023: 143.798

Text: 'Neural networks process information.'
  Active features: 128/16384 (0.8%)
  Top 10 features:
    Feature 13092: 495.467
    Feature 12441: 467.829
    Feature 5603: 423.164
    Feature 9061: 400.580
    Feature 12315: 392.647
    Feature 10253: 333.114
    Feature 13388: 319.253
    Feature 7915: 289.497
    Feature 11322: 202.188
    Feature 8281: 179.745

Text: 'London is a major city in England.'
  Active features: 147/16384 (0.9%)
  Top 10 features:
    Feature 13092: 275.356
    Feature 12441: 268.304
    Feature 5603: 253.618
    Feature 9061: 224.818
    Feature 10253: 217.944
    Feature 7915: 201.600
    Feature 12315: 184.458
    Feature 4831: 159.930
    Feature 11872: 151.408
    Feature 2014: 144.797
============================================================

Saving checkpoint...
✓ Checkpoint saved to: C:\Users\paras\PycharmProjects\SAE_play\src\sae\experiments\checkpoints\deep_sae_layer3
  - Weights: sae_weights.pt
  - Activation mean: activation_mean.pt
  - Config: config.json

============================================================
EXPERIMENT COMPLETE!
============================================================

Deep SAE trained successfully!
Architecture: 2-layer encoder, 2-layer decoder
Latent dimension: 16,384
Final reconstruction loss: 0.0355
Final sparsity: 0.4% active features

Process finished with exit code 0